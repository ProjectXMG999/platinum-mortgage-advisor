"""
Kompleksowy benchmark wszystkich modeli AI
Cel: Ranking wydajnoÅ›ci dla kaÅ¼dego modelu w ETAP 1 i ETAP 2

Testuje:
- 6 modeli: gpt-4.1, gpt-4.1-nano, gpt-5-mini, gpt-5-nano, o1, o4-mini
- 3 profile klientÃ³w (simple, medium, complex)
- Osobno ETAP 1 (walidacja) i ETAP 2 (ranking)
- Tryb async parallel vs sequential

Wynik: Ranking modeli wedÅ‚ug szybkoÅ›ci dla kaÅ¼dego etapu
"""

import time
import sys
import os
import json
from datetime import datetime
from typing import Dict, List, Tuple
import asyncio

# Dodaj src do Å›cieÅ¼ki
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.query_engine import QueryEngine
from src.ai_client import AIClient
from src.data_processor import DataProcessor

# DostÄ™pne modele
AVAILABLE_MODELS = [
    "gpt-4.1",
    "gpt-4.1-nano",
    "gpt-5-mini",
    "gpt-5-nano",
    "o1",
    "o4-mini"
]

# Profile testowe
TEST_PROFILES = {
    "simple": {
        "name": "MÅ‚ode maÅ‚Å¼eÅ„stwo - zakup mieszkania",
        "query": """MÅ‚ode maÅ‚Å¼eÅ„stwo (28 i 30 lat) chce kupiÄ‡ mieszkanie za 600,000 zÅ‚. 
WkÅ‚ad wÅ‚asny: 150,000 zÅ‚. Oboje na umowie o pracÄ™ na czas nieokreÅ›lony (staÅ¼ 4 lata). 
WspÃ³lne dochody: 15,000 zÅ‚ netto/miesiÄ…c. Brak innych kredytÃ³w."""
    },
    "medium": {
        "name": "Senior - zakup dziaÅ‚ki pod budowÄ™",
        "query": """Klient (55 lat) planuje zakup dziaÅ‚ki budowlanej (1200 m2) za 250,000 zÅ‚ 
i budowÄ™ domu systemem zleconym (koszt 800,000 zÅ‚). WkÅ‚ad wÅ‚asny: 400,000 zÅ‚. 
DochÃ³d z dziaÅ‚alnoÅ›ci gospodarczej (peÅ‚na ksiÄ™gowoÅ›Ä‡, 8 lat): 18,000 zÅ‚ netto. 
Jeden kredyt konsumpcyjny: 500 zÅ‚/mc."""
    },
    "complex": {
        "name": "Inwestor - kamienica z lokalami",
        "query": """Inwestor (42 lata) kupuje kamienicÄ™ z 3 lokalami uÅ¼ytkowymi za 2,500,000 zÅ‚. 
WkÅ‚ad wÅ‚asny: 750,000 zÅ‚. Dochody: umowa o pracÄ™ (12,000 zÅ‚) + wynajem 2 mieszkaÅ„ (6,000 zÅ‚) 
+ dywidendy ze spÃ³Å‚ki (4,000 zÅ‚, otrzymywane 3 lata). Transakcja rodzinna (brat sprzedaje). 
Kredyt hipoteczny w PKO BP: 800,000 zÅ‚ (500 zÅ‚/mc pozostaÅ‚o)."""
    }
}


class ModelBenchmark:
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "models_tested": AVAILABLE_MODELS,
            "profiles_tested": list(TEST_PROFILES.keys()),
            "etap1_results": {},  # model -> [times]
            "etap2_results": {},  # model -> [times]
            "etap1_ranking": [],
            "etap2_ranking": [],
            "detailed_results": []
        }
        
        # Inicjalizuj QueryEngine raz
        print("\n" + "="*80)
        print("ğŸš€ INICJALIZACJA SYSTEMU")
        print("="*80)
        self.engine = QueryEngine("data/processed/knowledge_base.json")
        print("âœ… System gotowy\n")
    
    def test_model_etap1(
        self, 
        model_name: str, 
        profile_name: str, 
        query: str,
        use_async: bool = True
    ) -> Tuple[float, int, bool]:
        """
        Testuje model w ETAP 1 (Walidacja WYMOGÃ“W)
        
        Returns:
            Tuple[czas, liczba_zakwalifikowanych, success]
        """
        knowledge_base = self.engine.data_processor.format_compact_for_context()
        knowledge_base_dict = self.engine.data_processor.knowledge_base
        
        print(f"  ğŸ” ETAP 1: {model_name} | {profile_name} | {'ASYNC' if use_async else 'SYNC'}")
        
        start_time = time.time()
        
        try:
            if use_async:
                # Async parallel processing
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    validation_response, validation_data = loop.run_until_complete(
                        self.engine.ai_client.validate_requirements_async(
                            user_query=query,
                            knowledge_base=knowledge_base_dict,
                            deployment_name=model_name
                        )
                    )
                finally:
                    loop.close()
            else:
                # Sequential processing
                validation_response, validation_data = self.engine.ai_client.validate_requirements(
                    user_query=query,
                    knowledge_base_context=knowledge_base
                )
            
            elapsed = time.time() - start_time
            
            # Zlicz zakwalifikowane banki
            qualified_count = 0
            if validation_data and "qualified_banks" in validation_data:
                qualified_count = len(validation_data["qualified_banks"])
            
            print(f"    âœ… {elapsed:.2f}s | Zakwalifikowane: {qualified_count}/11")
            
            return elapsed, qualified_count, True
            
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"    âŒ {elapsed:.2f}s | BÅÄ„D: {str(e)[:100]}")
            return elapsed, 0, False
    
    def test_model_etap2(
        self, 
        model_name: str, 
        profile_name: str, 
        query: str,
        qualified_banks: List[str]
    ) -> Tuple[float, bool]:
        """
        Testuje model w ETAP 2 (Ranking JAKOÅšCI)
        
        Returns:
            Tuple[czas, success]
        """
        if len(qualified_banks) == 0:
            print(f"  ğŸ… ETAP 2: {model_name} | {profile_name} | SKIP (brak bankÃ³w)")
            return 0, False
        
        knowledge_base = self.engine.data_processor.format_compact_for_context()
        
        print(f"  ğŸ… ETAP 2: {model_name} | {profile_name} | {len(qualified_banks)} bankÃ³w")
        
        start_time = time.time()
        
        try:
            ranking_response = self.engine.ai_client.rank_by_quality(
                user_query=query,
                knowledge_base_context=knowledge_base,
                qualified_banks=qualified_banks,
                deployment_name=model_name
            )
            
            elapsed = time.time() - start_time
            print(f"    âœ… {elapsed:.2f}s")
            
            return elapsed, True
            
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"    âŒ {elapsed:.2f}s | BÅÄ„D: {str(e)[:100]}")
            return elapsed, False
    
    def run_comprehensive_benchmark(self, use_async: bool = True):
        """
        Uruchamia kompletny benchmark dla wszystkich modeli i profili
        """
        print("\n" + "="*80)
        print("ğŸ“Š KOMPLEKSOWY BENCHMARK WSZYSTKICH MODELI")
        print("="*80)
        print(f"Modele: {len(AVAILABLE_MODELS)}")
        print(f"Profile: {len(TEST_PROFILES)}")
        print(f"Tryb: {'ASYNC PARALLEL' if use_async else 'SEQUENTIAL'}")
        print(f"ÅÄ…cznie testÃ³w: {len(AVAILABLE_MODELS) * len(TEST_PROFILES) * 2} (ETAP 1 + ETAP 2)")
        print("="*80 + "\n")
        
        # Inicjalizuj wyniki dla kaÅ¼dego modelu
        for model in AVAILABLE_MODELS:
            self.results["etap1_results"][model] = []
            self.results["etap2_results"][model] = []
        
        # Iteruj przez wszystkie kombinacje
        for model in AVAILABLE_MODELS:
            print(f"\n{'='*80}")
            print(f"ğŸ¤– TESTOWANIE MODELU: {model}")
            print(f"{'='*80}\n")
            
            for profile_key, profile_data in TEST_PROFILES.items():
                profile_name = profile_data["name"]
                query = profile_data["query"]
                
                print(f"\nğŸ“‹ Profil: {profile_name}")
                print(f"{'â”€'*80}")
                
                # ETAP 1: Walidacja
                etap1_time, qualified_count, etap1_success = self.test_model_etap1(
                    model_name=model,
                    profile_name=profile_name,
                    query=query,
                    use_async=use_async
                )
                
                if etap1_success:
                    self.results["etap1_results"][model].append(etap1_time)
                
                # ETAP 2: Ranking (tylko jeÅ›li mamy zakwalifikowane banki)
                if qualified_count > 0 and etap1_success:
                    # Dla uproszczenia uÅ¼ywamy TOP 4 bankÃ³w (symulacja)
                    mock_qualified = ["Alior Bank", "PKO BP", "mBank", "Millennium"][:qualified_count]
                    
                    etap2_time, etap2_success = self.test_model_etap2(
                        model_name=model,
                        profile_name=profile_name,
                        query=query,
                        qualified_banks=mock_qualified
                    )
                    
                    if etap2_success:
                        self.results["etap2_results"][model].append(etap2_time)
                else:
                    etap2_time = 0
                    etap2_success = False
                
                # Zapisz szczegÃ³Å‚owe wyniki
                self.results["detailed_results"].append({
                    "model": model,
                    "profile": profile_name,
                    "profile_key": profile_key,
                    "etap1_time": etap1_time,
                    "etap1_success": etap1_success,
                    "qualified_count": qualified_count,
                    "etap2_time": etap2_time,
                    "etap2_success": etap2_success,
                    "total_time": etap1_time + etap2_time,
                    "use_async": use_async
                })
                
                print(f"  ğŸ’° TOTAL: {etap1_time + etap2_time:.2f}s (ETAP 1: {etap1_time:.2f}s + ETAP 2: {etap2_time:.2f}s)")
        
        # Oblicz ranking
        self.calculate_rankings()
        
        # WyÅ›wietl wyniki
        self.display_results()
        
        # Zapisz do pliku
        self.save_results()
    
    def calculate_rankings(self):
        """Oblicza rankingi modeli wedÅ‚ug Å›redniego czasu"""
        print("\n" + "="*80)
        print("ğŸ“Š OBLICZANIE RANKINGÃ“W")
        print("="*80 + "\n")
        
        # Ranking ETAP 1
        etap1_avg = []
        for model, times in self.results["etap1_results"].items():
            if len(times) > 0:
                avg_time = sum(times) / len(times)
                etap1_avg.append({
                    "model": model,
                    "avg_time": round(avg_time, 3),
                    "min_time": round(min(times), 3),
                    "max_time": round(max(times), 3),
                    "tests_count": len(times)
                })
        
        self.results["etap1_ranking"] = sorted(etap1_avg, key=lambda x: x["avg_time"])
        
        # Ranking ETAP 2
        etap2_avg = []
        for model, times in self.results["etap2_results"].items():
            if len(times) > 0:
                avg_time = sum(times) / len(times)
                etap2_avg.append({
                    "model": model,
                    "avg_time": round(avg_time, 3),
                    "min_time": round(min(times), 3),
                    "max_time": round(max(times), 3),
                    "tests_count": len(times)
                })
        
        self.results["etap2_ranking"] = sorted(etap2_avg, key=lambda x: x["avg_time"])
    
    def display_results(self):
        """WyÅ›wietla czytelne podsumowanie wynikÃ³w"""
        print("\n" + "="*80)
        print("ğŸ† RANKING MODELI - ETAP 1 (WALIDACJA WYMOGÃ“W)")
        print("="*80 + "\n")
        
        medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£", "6ï¸âƒ£"]
        
        for idx, result in enumerate(self.results["etap1_ranking"]):
            medal = medals[idx] if idx < len(medals) else f"{idx+1}."
            print(f"{medal} {result['model']:15} | "
                  f"AVG: {result['avg_time']:6.2f}s | "
                  f"MIN: {result['min_time']:6.2f}s | "
                  f"MAX: {result['max_time']:6.2f}s | "
                  f"Tests: {result['tests_count']}")
        
        print("\n" + "="*80)
        print("ğŸ† RANKING MODELI - ETAP 2 (RANKING JAKOÅšCI)")
        print("="*80 + "\n")
        
        for idx, result in enumerate(self.results["etap2_ranking"]):
            medal = medals[idx] if idx < len(medals) else f"{idx+1}."
            print(f"{medal} {result['model']:15} | "
                  f"AVG: {result['avg_time']:6.2f}s | "
                  f"MIN: {result['min_time']:6.2f}s | "
                  f"MAX: {result['max_time']:6.2f}s | "
                  f"Tests: {result['tests_count']}")
        
        # Najlepsza kombinacja
        print("\n" + "="*80)
        print("ğŸ’¡ REKOMENDOWANE KOMBINACJE")
        print("="*80 + "\n")
        
        if self.results["etap1_ranking"] and self.results["etap2_ranking"]:
            best_etap1 = self.results["etap1_ranking"][0]
            best_etap2 = self.results["etap2_ranking"][0]
            
            print(f"ğŸš€ NAJSZYBSZY CZAS CAÅKOWITY:")
            print(f"   ETAP 1: {best_etap1['model']} ({best_etap1['avg_time']:.2f}s)")
            print(f"   ETAP 2: {best_etap2['model']} ({best_etap2['avg_time']:.2f}s)")
            print(f"   TOTAL:  {best_etap1['avg_time'] + best_etap2['avg_time']:.2f}s")
            
            # SprawdÅº czy osiÄ…gniÄ™to cel <15s
            total_time = best_etap1['avg_time'] + best_etap2['avg_time']
            if total_time < 15:
                print(f"\n   âœ… CEL OSIÄ„GNIÄ˜TY! ({15 - total_time:.2f}s poniÅ¼ej limitu 15s)")
            else:
                print(f"\n   âš ï¸ CEL NIEOSIÄ„GNIÄ˜TY ({total_time - 15:.2f}s powyÅ¼ej limitu 15s)")
    
    def save_results(self):
        """Zapisuje wyniki do pliku JSON"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_results/model_benchmark_{timestamp}.json"
        
        os.makedirs("test_results", exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Wyniki zapisane: {filename}")
        
        # Zapisz rÃ³wnieÅ¼ czytelnÄ… wersjÄ™ markdown
        md_filename = f"test_results/model_benchmark_{timestamp}.md"
        self.save_markdown_report(md_filename)
        print(f"ğŸ“„ Raport markdown: {md_filename}")
    
    def save_markdown_report(self, filename: str):
        """Zapisuje czytelny raport w markdown"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("# Benchmark Modeli AI - Platinum Mortgage Advisor\n\n")
            f.write(f"**Data:** {self.results['timestamp']}\n\n")
            
            f.write("## ğŸ† Ranking ETAP 1 (Walidacja WYMOGÃ“W)\n\n")
            f.write("| Pozycja | Model | AVG Time | MIN Time | MAX Time | Testy |\n")
            f.write("|---------|-------|----------|----------|----------|---------|\n")
            
            medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£", "6ï¸âƒ£"]
            for idx, result in enumerate(self.results["etap1_ranking"]):
                medal = medals[idx] if idx < len(medals) else f"{idx+1}"
                f.write(f"| {medal} | {result['model']} | {result['avg_time']:.2f}s | "
                       f"{result['min_time']:.2f}s | {result['max_time']:.2f}s | "
                       f"{result['tests_count']} |\n")
            
            f.write("\n## ğŸ† Ranking ETAP 2 (Ranking JAKOÅšCI)\n\n")
            f.write("| Pozycja | Model | AVG Time | MIN Time | MAX Time | Testy |\n")
            f.write("|---------|-------|----------|----------|----------|---------|\n")
            
            for idx, result in enumerate(self.results["etap2_ranking"]):
                medal = medals[idx] if idx < len(medals) else f"{idx+1}"
                f.write(f"| {medal} | {result['model']} | {result['avg_time']:.2f}s | "
                       f"{result['min_time']:.2f}s | {result['max_time']:.2f}s | "
                       f"{result['tests_count']} |\n")
            
            f.write("\n## ğŸ“Š SzczegÃ³Å‚owe wyniki\n\n")
            f.write("| Model | Profil | ETAP 1 | ETAP 2 | TOTAL | Status |\n")
            f.write("|-------|--------|--------|--------|-------|---------|\n")
            
            for result in self.results["detailed_results"]:
                status = "âœ…" if result["etap1_success"] and result["etap2_success"] else "âš ï¸"
                f.write(f"| {result['model']} | {result['profile'][:30]} | "
                       f"{result['etap1_time']:.2f}s | {result['etap2_time']:.2f}s | "
                       f"{result['total_time']:.2f}s | {status} |\n")


def main():
    """GÅ‚Ã³wna funkcja benchmarku"""
    print("\n" + "="*80)
    print("ğŸš€ KOMPLEKSOWY BENCHMARK MODELI AI")
    print("   Platinum Mortgage Advisor - Performance Testing")
    print("="*80)
    
    benchmark = ModelBenchmark()
    
    # WybÃ³r trybu
    print("\nWybierz tryb testowania:")
    print("1. ASYNC PARALLEL (szybki - rekomendowany)")
    print("2. SEQUENTIAL (wolny - dla porÃ³wnania)")
    
    choice = input("\nWybÃ³r (1/2) [domyÅ›lnie 1]: ").strip() or "1"
    use_async = choice == "1"
    
    # Uruchom benchmark
    benchmark.run_comprehensive_benchmark(use_async=use_async)
    
    print("\n" + "="*80)
    print("âœ… BENCHMARK ZAKOÅƒCZONY")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
